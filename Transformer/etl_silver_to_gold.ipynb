{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99dfc74a",
   "metadata": {},
   "source": [
    "# ETL Silver -> Gold\n",
    "Pipeline de transforma√ß√£o de dados da camada Silver para o Data Warehouse (Gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8c59d1",
   "metadata": {},
   "source": [
    "## 1. Importa√ß√µes e Configura√ß√£o Inicial\n",
    "Importa√ß√£o de bibliotecas necess√°rias, configura√ß√£o de par√¢metros e leitura do DDL externo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac056aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CONFIGURA√á√ÉO E IN√çCIO\n",
    "# ==============================================================================\n",
    "print(\"üèÜ Iniciando ETL Gold (Silver -> DW)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# CONFIGURA√á√ïES DE SCHEMA\n",
    "GOLD_SCHEMA = \"DW\"           # Nome do Schema de destino (Mai√∫sculo)\n",
    "SILVER_TABLE = \"silver.acd\"  # Tabela de origem\n",
    "DDL_FILE_NAME = \"ddl.sql\"\n",
    "\n",
    "db_name = \"acidentes_db\"\n",
    "db_user = \"postgres\"\n",
    "db_pass = \"admin\"\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. LEITURA DO ARQUIVO DDL EXTERNO\n",
    "# ==============================================================================\n",
    "def ler_ddl_externo(nome_arquivo):\n",
    "    caminhos_tentativa = [\n",
    "        os.path.join(\"gold\", nome_arquivo),                       \n",
    "        os.path.join(\"Data_Layer\", \"gold\", nome_arquivo),         \n",
    "        os.path.join(\"..\", \"Data_Layer\", \"gold\", nome_arquivo),   \n",
    "        f\"/home/jovyan/work/Data_Layer/gold/{nome_arquivo}\"       \n",
    "    ]\n",
    "    \n",
    "    for caminho in caminhos_tentativa:\n",
    "        if os.path.exists(caminho):\n",
    "            print(f\"üìú Arquivo DDL encontrado: {caminho}\")\n",
    "            try:\n",
    "                with open(caminho, 'r', encoding='utf-8') as f:\n",
    "                    return f.read()\n",
    "            except:\n",
    "                with open(caminho, 'r', encoding='latin-1') as f:\n",
    "                    return f.read()\n",
    "    \n",
    "    print(f\"‚ùå ERRO: Arquivo '{nome_arquivo}' n√£o encontrado.\")\n",
    "    return None\n",
    "\n",
    "DDL_CONTENT = ler_ddl_externo(DDL_FILE_NAME)\n",
    "\n",
    "if not DDL_CONTENT:\n",
    "    raise FileNotFoundError(f\"Pare! O arquivo {DDL_FILE_NAME} precisa existir na pasta gold.\")\n",
    "\n",
    "# --- TRUQUE: SUBSTITUI√á√ÉO DIN√ÇMICA COM ASPAS ---\n",
    "# Garante que o SQL use \"DW\" (com aspas) para respeitar a caixa alta\n",
    "DDL_FINAL = DDL_CONTENT.replace(\"gold.\", f'\"{GOLD_SCHEMA}\".')\n",
    "DDL_FINAL = DDL_FINAL.replace(\"DW.\", f'\"{GOLD_SCHEMA}\".') \n",
    "\n",
    "# Remove comandos de cria√ß√£o de schema antigos do arquivo para evitar erros\n",
    "DDL_FINAL = DDL_FINAL.replace(\"CREATE SCHEMA IF NOT EXISTS gold\", \"\") \n",
    "DDL_FINAL = DDL_FINAL.replace(\"CREATE SCHEMA IF NOT EXISTS DW\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb52a45a",
   "metadata": {},
   "source": [
    "## 2. Conex√£o e Prepara√ß√£o do Schema\n",
    "Estabelecimento de conex√£o com o banco de dados e cria√ß√£o do schema DW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ec8e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = None\n",
    "print(\"\\nüîå Conectando ao banco de dados...\")\n",
    "\n",
    "def tentar_conexao(url, nome):\n",
    "    try:\n",
    "        eng = create_engine(url)\n",
    "        with eng.connect() as conn:\n",
    "            print(f\"‚úÖ Conectado via {nome}\")\n",
    "            \n",
    "            # 1. Garante Schema (COM ASPAS PARA FOR√áAR MAI√öSCULO)\n",
    "            print(f\"üõ†Ô∏è Criando/Verificando schema '\\\"{GOLD_SCHEMA}\\\"'...\")\n",
    "            conn.execute(text(f'CREATE SCHEMA IF NOT EXISTS \"{GOLD_SCHEMA}\";'))\n",
    "            conn.commit()\n",
    "            \n",
    "            # 2. Roda DDL\n",
    "            print(\"üîÑ Executando DDL (Recriando tabelas)...\")\n",
    "            conn.execute(text(DDL_FINAL))\n",
    "            conn.commit()\n",
    "        return eng\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Falha na conex√£o {nome}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Tenta Localhost PRIMEIRO\n",
    "db_url_local = f\"postgresql+psycopg2://{db_user}:{db_pass}@localhost:5432/{db_name}\"\n",
    "engine = tentar_conexao(db_url_local, \"LOCALHOST\")\n",
    "\n",
    "# Tenta Docker como fallback\n",
    "if not engine:\n",
    "    print(\"‚ö†Ô∏è Localhost falhou. Tentando Docker...\")\n",
    "    db_url_docker = f\"postgresql+psycopg2://{db_user}:{db_pass}@db:5432/{db_name}\"\n",
    "    engine = tentar_conexao(db_url_docker, \"DOCKER\")\n",
    "\n",
    "if not engine:\n",
    "    raise ConnectionError(\"‚ùå ERRO CR√çTICO: Falha total de conex√£o com o banco.\")\n",
    "\n",
    "print(f\"‚úÖ Estrutura '{GOLD_SCHEMA}' Pronta!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca0dce3",
   "metadata": {},
   "source": [
    "## 3. Extra√ß√£o (EXTRACTION)\n",
    "Leitura dos dados da tabela Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df322772",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüì• Lendo tabela Silver: {SILVER_TABLE}\")\n",
    "try:\n",
    "    df_silver = pd.read_sql(f\"SELECT * FROM {SILVER_TABLE}\", engine)\n",
    "    print(f\"‚úÖ Registros carregados: {len(df_silver)}\")\n",
    "    \n",
    "    if len(df_silver) == 0: raise SystemExit(\"Silver Vazia\")\n",
    "\n",
    "    if 'ttl_aer_env' not in df_silver.columns:\n",
    "        df_silver['ttl_aer_env'] = 1\n",
    "\n",
    "    # --- GARANTIA DE TIPOS ---\n",
    "    print(\"   -> Garantindo tipagem num√©rica para Lat/Lon...\")\n",
    "    df_silver['lat'] = pd.to_numeric(df_silver['lat'], errors='coerce').fillna(0.0)\n",
    "    df_silver['lon'] = pd.to_numeric(df_silver['lon'], errors='coerce').fillna(0.0)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro ao ler Silver: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2e5b4d",
   "metadata": {},
   "source": [
    "## 4. Transforma√ß√£o (TRANSFORMATION)\n",
    "Constru√ß√£o das dimens√µes do Data Warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9779d7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dimension(df_unique, table_name):\n",
    "    if df_unique.empty: return\n",
    "    try:\n",
    "        table_simple = table_name.split('.')[-1]\n",
    "        print(f\"---> Carga: {GOLD_SCHEMA}.{table_simple} ({len(df_unique)} linhas)\")\n",
    "        \n",
    "        df_unique.to_sql(\n",
    "            name=table_simple, \n",
    "            schema=GOLD_SCHEMA, \n",
    "            con=engine, \n",
    "            if_exists='append', \n",
    "            index=False\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro em {table_name}: {e}\")\n",
    "        raise e\n",
    "\n",
    "print(\"\\nüî® Construindo Dimens√µes...\")\n",
    "\n",
    "# 1. Aeronave\n",
    "df_aer = df_silver[['mat_aer', 'fab_aer', 'mdl_aer', 'tpo_aer']].drop_duplicates().copy()\n",
    "df_aer.columns = ['cod_mat', 'nom_fab', 'nom_mdl', 'des_tpo']\n",
    "save_dimension(df_aer, f\"{GOLD_SCHEMA}.dim_aer\")\n",
    "\n",
    "# 2. Localiza√ß√£o\n",
    "df_loc = df_silver[['mun', 'uf', 'lat', 'lon']].drop_duplicates().copy()\n",
    "df_loc.columns = ['nom_mun', 'sgl_uf', 'num_lat', 'num_lon']\n",
    "save_dimension(df_loc, f\"{GOLD_SCHEMA}.dim_loc\")\n",
    "\n",
    "# 3. Tempo\n",
    "df_tmp = df_silver[['ano', 'mes', 'dia']].drop_duplicates().copy()\n",
    "df_tmp.columns = ['num_ano', 'num_mes', 'num_dia']\n",
    "save_dimension(df_tmp, f\"{GOLD_SCHEMA}.dim_tmp\")\n",
    "\n",
    "# 4. Ocorr√™ncia\n",
    "cols_ocr = ['cod_ocr', 'cls_ocr', 'tpo_ocr', 'fse_ope', 'nvl_sev', 'nvl_dno']\n",
    "df_ocr = df_silver[cols_ocr].drop_duplicates().copy()\n",
    "df_ocr.columns = ['cod_ocr', 'des_cls', 'des_tpo', 'des_fse', 'des_sev', 'des_dno']\n",
    "save_dimension(df_ocr, f\"{GOLD_SCHEMA}.dim_ocr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec29c23c",
   "metadata": {},
   "source": [
    "## 5. Carga no Data Warehouse (LOADING)\n",
    "Constru√ß√£o da tabela fato atrav√©s de joins com as dimens√µes e carga final no DW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1986a300",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîó Cruzando dados para Fato...\")\n",
    "\n",
    "try:\n",
    "    # AQUI EST√Å A CORRE√á√ÉO PRINCIPAL: \n",
    "    # Usamos aspas simples (') fora e aspas duplas (\") dentro para o schema\n",
    "    # Ex: SELECT * FROM \"DW\".dim_aer\n",
    "    \n",
    "    print(f\"   -> Lendo dimens√µes do schema \\\"{GOLD_SCHEMA}\\\"...\")\n",
    "    dim_aer = pd.read_sql(f'SELECT * FROM \"{GOLD_SCHEMA}\".dim_aer', engine)\n",
    "    dim_loc = pd.read_sql(f'SELECT * FROM \"{GOLD_SCHEMA}\".dim_loc', engine)\n",
    "    dim_tmp = pd.read_sql(f'SELECT * FROM \"{GOLD_SCHEMA}\".dim_tmp', engine)\n",
    "    dim_ocr = pd.read_sql(f'SELECT * FROM \"{GOLD_SCHEMA}\".dim_ocr', engine)\n",
    "\n",
    "    # Garante tipos float para evitar erro no merge\n",
    "    dim_loc['num_lat'] = pd.to_numeric(dim_loc['num_lat'], errors='coerce').fillna(0.0)\n",
    "    dim_loc['num_lon'] = pd.to_numeric(dim_loc['num_lon'], errors='coerce').fillna(0.0)\n",
    "\n",
    "    # Merge\n",
    "    print(\"   -> Realizando Joins...\")\n",
    "    df_fato = df_silver.merge(\n",
    "        dim_aer, left_on=['mat_aer', 'fab_aer', 'mdl_aer', 'tpo_aer'], right_on=['cod_mat', 'nom_fab', 'nom_mdl', 'des_tpo']\n",
    "    ).merge(\n",
    "        dim_loc, left_on=['mun', 'uf', 'lat', 'lon'], right_on=['nom_mun', 'sgl_uf', 'num_lat', 'num_lon']\n",
    "    ).merge(\n",
    "        dim_tmp, left_on=['ano', 'mes', 'dia'], right_on=['num_ano', 'num_mes', 'num_dia']\n",
    "    ).merge(\n",
    "        dim_ocr, left_on=['cod_ocr', 'cls_ocr', 'tpo_ocr', 'fse_ope', 'nvl_sev', 'nvl_dno'], right_on=['cod_ocr', 'des_cls', 'des_tpo', 'des_fse', 'des_sev', 'des_dno']\n",
    "    )\n",
    "\n",
    "    # Sele√ß√£o\n",
    "    df_fato_final = df_fato[[\n",
    "        'srk_aer', 'srk_loc', 'srk_tmp', 'srk_ocr', \n",
    "        'ttl_fat', 'ttl_rec', 'qtd_ase_aer', 'ttl_aer_env'\n",
    "    ]].copy()\n",
    "\n",
    "    df_fato_final.columns = ['srk_aer', 'srk_loc', 'srk_tmp', 'srk_ocr', 'num_fat', 'num_rec', 'num_ase', 'num_env']\n",
    "\n",
    "    print(f\"\\nüíæ Salvando Fato (\\\"{GOLD_SCHEMA}\\\".fat_ocr)...\")\n",
    "    \n",
    "    df_fato_final.to_sql(\n",
    "        name='fat_ocr', \n",
    "        schema=GOLD_SCHEMA, \n",
    "        con=engine, \n",
    "        if_exists='append', \n",
    "        index=False, \n",
    "        chunksize=2000\n",
    "    )\n",
    "    print(f\"‚úÖ SUCESSO! {len(df_fato_final)} registros carregados no DW.\")\n",
    "    print(f\"üöÄ Tempo total: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro na etapa final: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
