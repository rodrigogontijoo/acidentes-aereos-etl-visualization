{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91bd2f08",
   "metadata": {},
   "source": [
    "# ETL Raw -> Silver\n",
    "Pipeline de transforma√ß√£o de dados brutos para a camada Silver (Acidentes A√©reos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f2883b",
   "metadata": {},
   "source": [
    "## 1. Importa√ß√µes e Configura√ß√£o Inicial\n",
    "Importa√ß√£o de bibliotecas necess√°rias e inicializa√ß√£o do processo ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41e23983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üáßüá∑ Iniciando ETL Raw -> Silver (Acidentes A√©reos)...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CONFIGURA√á√ÉO E IN√çCIO\n",
    "# ==============================================================================\n",
    "print(\"üáßüá∑ Iniciando ETL Raw -> Silver (Acidentes A√©reos)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Fun√ß√£o auxiliar para encontrar arquivos (Robustez de path)\n",
    "def encontrar_arquivo(nome_arquivo):\n",
    "    possible_paths = [\n",
    "        nome_arquivo,\n",
    "        os.path.join('Data_Layer', 'raw', nome_arquivo),\n",
    "        os.path.join('raw', nome_arquivo),\n",
    "        os.path.join('..', 'Data_Layer', 'raw', nome_arquivo),\n",
    "        f\"/home/jovyan/work/Data_Layer/raw/{nome_arquivo}\" \n",
    "    ]\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            return path\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7b4067",
   "metadata": {},
   "source": [
    "## 2. Extra√ß√£o (EXTRACTION)\n",
    "Carregamento dos dados brutos do arquivo CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00bebac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Buscando arquivo de dados brutos...\n",
      "   -> Lendo Arquivo: ../Data_Layer/raw/data_raw.csv...\n",
      "‚úÖ Carga Raw Completa! Registros carregados: 6114\n"
     ]
    }
   ],
   "source": [
    "print(\"üìÇ Buscando arquivo de dados brutos...\")\n",
    "\n",
    "nome_arquivo_raw = 'data_raw.csv'\n",
    "path_raw = encontrar_arquivo(nome_arquivo_raw)\n",
    "\n",
    "if not path_raw:\n",
    "    print(f\"‚ùå [ERRO CR√çTICO] Arquivo '{nome_arquivo_raw}' n√£o encontrado.\")\n",
    "    exit(1)\n",
    "\n",
    "try:\n",
    "    print(f\"   -> Lendo Arquivo: {path_raw}...\")\n",
    "    \n",
    "    df = pd.read_csv(path_raw, sep=';', encoding='utf-8', dtype=str)\n",
    "    \n",
    "    print(f\"‚úÖ Carga Raw Completa! Registros carregados: {len(df)}\")\n",
    "\n",
    "except UnicodeDecodeError:\n",
    "    print(\"‚ö†Ô∏è Falha com UTF-8. Tentando Latin-1...\")\n",
    "    df = pd.read_csv(path_raw, sep=';', encoding='latin1', dtype=str)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro ao ler CSV: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e37eb9",
   "metadata": {},
   "source": [
    "## 3. Transforma√ß√£o (TRANSFORMATION)\n",
    "Fase de limpeza, padroniza√ß√£o e remo√ß√£o de dados inv√°lidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5c6668d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ†Ô∏è Iniciando a limpeza e padroniza√ß√£o...\n",
      "   -> Processando datas...\n",
      "   -> Corrigindo Lat/Lon cient√≠fica...\n",
      "   -> Convertendo n√∫meros...\n",
      "   -> üßπ Removendo Outliers de Fatalidades...\n",
      "      [Estat√≠stica] Q1: 1.0 | Q3: 2.0 | Limite de Corte: 3.5\n",
      "      [Limpeza] Registros removidos (Outliers extremos): 64\n",
      "‚ú® Transforma√ß√£o conclu√≠da! Base Silver Final: 6050 registros.\n"
     ]
    }
   ],
   "source": [
    "print(\"üõ†Ô∏è Iniciando a limpeza e padroniza√ß√£o...\")\n",
    "df_silver = df.copy()\n",
    "\n",
    "# 4.1 RENOMEA√á√ÉO (Mapeamento Raw -> SQL Schema)\n",
    "mapa_colunas = {\n",
    "    'Codigo da Ocorrencia': 'cod_ocr',\n",
    "    'Classificacao da Ocorrencia ': 'cls_ocr',\n",
    "    'Classificacao da Ocorrencia': 'cls_ocr',\n",
    "    'Data e Hora da Ocorrencia': 'dta_ocr',\n",
    "    'Latitude da Ocorrencia': 'lat',\n",
    "    'Longitude da Ocorrencia': 'lon',\n",
    "    'Cidade da Ocorrencia': 'mun',\n",
    "    'UF da Ocorrencia': 'uf',\n",
    "    'Aerodromo da Ocorrencia': 'aer_ocr',\n",
    "    'Total de Recomendacoes': 'ttl_rec',\n",
    "    'Total de Aeronaves Envolvidas': 'ttl_aer_env',\n",
    "    'Ocorrencia na Saida da Pista?': 'sai_pst',\n",
    "    'Tipo de Ocorrencia': 'tpo_ocr',\n",
    "    'Matricula da Aeronave': 'mat_aer',\n",
    "    'Tipo de Aeronave': 'tpo_aer',\n",
    "    'Fabricante da Aeronave': 'fab_aer',\n",
    "    'Modelo de Aeronave': 'mdl_aer',\n",
    "    'Aeronave Motor Tipo': 'tpo_mtr',\n",
    "    'Quantidade de Assentos na Aeronave': 'qtd_ase_aer',\n",
    "    'Ano de Fabricacao da Aeronave': 'ano_fab_aer',\n",
    "    'Voo de Origem do Acidente': 'voo_ori',\n",
    "    'Voo Destino do Acidente': 'voo_dst',\n",
    "    'Fase de Operacao da Aeronave': 'fse_ope',\n",
    "    'Nivel de Dano da Aeronave': 'nvl_dno',\n",
    "    'Total de Fatalidades no Acidente': 'ttl_fat'\n",
    "}\n",
    "\n",
    "df_silver = df_silver.rename(columns=mapa_colunas)\n",
    "\n",
    "# 4.2 LIMPEZA DE TEXTO (Strings)\n",
    "cols_text = ['mun', 'uf', 'cls_ocr', 'tpo_ocr', 'mat_aer', 'tpo_aer', 'fab_aer', 'mdl_aer', 'fse_ope', 'nvl_dno']\n",
    "\n",
    "for col in cols_text:\n",
    "    if col in df_silver.columns:\n",
    "        df_silver[col] = df_silver[col].str.strip()\n",
    "        df_silver[col] = df_silver[col].replace(['****', '***', ''], np.nan)\n",
    "        df_silver[col] = df_silver[col].fillna(\"N√ÉO INFORMADO\")\n",
    "\n",
    "# 4.3 TRATAMENTO DE DATAS\n",
    "print(\"   -> Processando datas...\")\n",
    "df_silver['dta_ocr'] = pd.to_datetime(df_silver['dta_ocr'], format='%d/%m/%Y %H:%M', errors='coerce')\n",
    "df_silver['dta_ocr'] = df_silver['dta_ocr'].fillna(pd.Timestamp('1900-01-01 00:00:00'))\n",
    "\n",
    "df_silver['ano'] = df_silver['dta_ocr'].dt.year\n",
    "df_silver['mes'] = df_silver['dta_ocr'].dt.month\n",
    "df_silver['dia'] = df_silver['dta_ocr'].dt.day\n",
    "df_silver['hor'] = df_silver['dta_ocr'].dt.hour\n",
    "\n",
    "# 4.4 TRATAMENTO DE COORDENADAS\n",
    "print(\"   -> Corrigindo Lat/Lon cient√≠fica...\")\n",
    "def limpar_coord(valor):\n",
    "    if pd.isna(valor) or valor == '': return 0.0\n",
    "    try:\n",
    "        val_str = str(valor).replace(',', '.')\n",
    "        val_float = float(val_str)\n",
    "        if abs(val_float) > 180: return 0.0\n",
    "        return val_float\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "if 'lat' in df_silver.columns: df_silver['lat'] = df_silver['lat'].apply(limpar_coord)\n",
    "if 'lon' in df_silver.columns: df_silver['lon'] = df_silver['lon'].apply(limpar_coord)\n",
    "\n",
    "# 4.5 TRATAMENTO NUM√âRICO\n",
    "print(\"   -> Convertendo n√∫meros...\")\n",
    "cols_int = ['ttl_fat', 'ttl_rec', 'ttl_aer_env', 'ano_fab_aer', 'qtd_ase_aer']\n",
    "\n",
    "for col in cols_int:\n",
    "    if col in df_silver.columns:\n",
    "        df_silver[col] = pd.to_numeric(df_silver[col], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# --- 4.6 REMO√á√ÉO DE OUTLIERS (L√ìGICA CORRIGIDA PARA DADOS ESPARSOS) ---\n",
    "print(\"   -> üßπ Removendo Outliers de Fatalidades...\")\n",
    "total_antes = len(df_silver)\n",
    "\n",
    "df_fatais = df_silver[df_silver['ttl_fat'] > 0]\n",
    "\n",
    "if not df_fatais.empty:\n",
    "    Q1 = df_fatais['ttl_fat'].quantile(0.25)\n",
    "    Q3 = df_fatais['ttl_fat'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    limite_superior = Q3 + 1.5 * IQR\n",
    "    \n",
    "    print(f\"      [Estat√≠stica] Q1: {Q1} | Q3: {Q3} | Limite de Corte: {limite_superior}\")\n",
    "    \n",
    "    df_silver = df_silver[\n",
    "        (df_silver['ttl_fat'] == 0) | \n",
    "        (df_silver['ttl_fat'] <= limite_superior)\n",
    "    ].copy()\n",
    "else:\n",
    "    print(\"      [Aviso] N√£o h√° fatalidades suficientes para c√°lculo de IQR.\")\n",
    "\n",
    "total_removido = total_antes - len(df_silver)\n",
    "print(f\"      [Limpeza] Registros removidos (Outliers extremos): {total_removido}\")\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# 4.7 SEVERIDADE (L√≥gica de Neg√≥cio)\n",
    "def calcular_severidade(fatais):\n",
    "    if fatais == 0: return 'LEVE'\n",
    "    elif fatais <= 10: return 'MODERADA'\n",
    "    elif fatais <= 50: return 'GRAVE'\n",
    "    else: return 'CRITICA'\n",
    "\n",
    "df_silver['nvl_sev'] = df_silver['ttl_fat'].apply(calcular_severidade)\n",
    "\n",
    "# 4.8 SELE√á√ÉO FINAL\n",
    "cols_finais = [\n",
    "    'cod_ocr', 'dta_ocr', 'ano', 'mes', 'dia', 'hor', \n",
    "    'uf', 'mun', 'lat', 'lon', \n",
    "    'cls_ocr', 'tpo_ocr', 'fse_ope', \n",
    "    'tpo_aer', 'fab_aer', 'mdl_aer', 'mat_aer', 'ano_fab_aer', 'qtd_ase_aer',\n",
    "    'nvl_dno', 'ttl_fat', 'ttl_rec', 'ttl_aer_env', 'nvl_sev'\n",
    "]\n",
    "cols_existentes = [c for c in cols_finais if c in df_silver.columns]\n",
    "df_final = df_silver[cols_existentes].copy()\n",
    "\n",
    "print(f\"‚ú® Transforma√ß√£o conclu√≠da! Base Silver Final: {len(df_final)} registros.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795c1a3b",
   "metadata": {},
   "source": [
    "## 4. Carga no Banco de Dados (LOADING)\n",
    "Conex√£o ao banco de dados, execu√ß√£o de DDL e inser√ß√£o dos dados transformados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "114a3cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîå Conectando ao Banco de Dados...\n",
      "‚úÖ Conectado via LOCALHOST\n",
      "üõ†Ô∏è Verificando schema 'silver'...\n",
      "‚úÖ Schema 'silver' garantido!\n",
      "üíæ Inserindo 6050 registros na tabela 'silver.acd'...\n",
      "‚úÖ SUCESSO! Dados carregados na tabela 'silver.acd'.\n",
      "\n",
      "üöÄ Job ETL Raw->Silver finalizado em 2.10 segundos!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüîå Conectando ao Banco de Dados...\")\n",
    "\n",
    "db_name = \"acidentes_db\"\n",
    "db_user = \"postgres\"\n",
    "db_pass = \"admin\"\n",
    "\n",
    "engine = None\n",
    "try:\n",
    "    url = f\"postgresql+psycopg2://{db_user}:{db_pass}@localhost:5432/{db_name}\"\n",
    "    engine = create_engine(url)\n",
    "    with engine.connect() as conn: pass\n",
    "    print(\"‚úÖ Conectado via LOCALHOST\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è Localhost falhou. Tentando Docker...\")\n",
    "    try:\n",
    "        url = f\"postgresql+psycopg2://{db_user}:{db_pass}@db:5432/{db_name}\"\n",
    "        engine = create_engine(url)\n",
    "        with engine.connect() as conn: pass\n",
    "        print(\"‚úÖ Conectado via DOCKER\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro fatal de conex√£o: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        print(\"üõ†Ô∏è Verificando schema 'silver'...\")\n",
    "        conn.execute(text(\"CREATE SCHEMA IF NOT EXISTS silver;\"))\n",
    "        conn.commit()\n",
    "        print(\"‚úÖ Schema 'silver' garantido!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro ao criar schema: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# INSER√á√ÉO\n",
    "try:\n",
    "    print(f\"üíæ Inserindo {len(df_final)} registros na tabela 'silver.acd'...\")\n",
    "    \n",
    "    df_final.to_sql(\n",
    "        name='acd',         \n",
    "        schema='silver',    \n",
    "        con=engine,\n",
    "        if_exists='replace', \n",
    "        index=False,\n",
    "        chunksize=1000,\n",
    "        method='multi'\n",
    "    )\n",
    "    print(\"‚úÖ SUCESSO! Dados carregados na tabela 'silver.acd'.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro na inser√ß√£o: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "print(f\"\\nüöÄ Job ETL Raw->Silver finalizado em {time.time() - start_time:.2f} segundos!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
